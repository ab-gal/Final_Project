import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


df_original = pd.read_csv(r"..\1_Data\london_house_price_data.csv")
df_subset = pd.read_csv(r'../1_data/df_lbsm_cleaned.csv')


df_merged = pd.merge(df_original, df_subset, on='postcode', how='left')

# 5. Optional: Fill missing values if some postcodes didn't match the building data
df_merged['construction_age_band'] = df_merged['construction_age_band'].fillna('Unknown')
df_merged['conservation_area_flag'] = df_merged['conservation_area_flag'].fillna('not in conservation area')
df_original = df_merged.copy()


df_original.columns


df_original['conservation_area_flag'].value_counts()


pd.set_option('display.max_columns', None)
df_original


df = df_original

df['in_conservation_area'] = (df['conservation_area_flag'] == 'in conservation area').astype(int)
df = df.drop(columns=['conservation_area_flag'])

df['soldPrice'] = df ['history_price']
df['soldDate'] = df ['history_date']
df['soldDate'] = pd.to_datetime(df['soldDate'])
df['soldYear'] = df['soldDate'].dt.year
df['sqmPrice'] = (df['soldPrice']/df['floorAreaSqM']).round(2)
# Bins
bins = [0, 37, 50, 60, 72, 90, 120, 160, 200, 10000]
labels = ['< 37m²', '37-50m²', '50-60m²', '60-72m²', '72-90m²', '90m²-120m²', '120m²-160m²', '160m²-200m²', '> 200m²']
df['size_bucket'] = pd.cut(df['floorAreaSqM'], bins=bins, labels=labels)

df = df[df['soldYear'] >= 2023]
def get_trimester(month):
    if month in [1, 2, 3]:
        return 'T1'
    elif month in [4, 5, 6]:
        return 'T2'
    elif month in [7, 8, 9]:
        return 'T3'
    else:
        return 'T4'

df['soldT'] = df['soldDate'].dt.month.apply(get_trimester)

column_order = ['fullAddress', 'postcode', 'country', 'outcode', 'latitude',
       'longitude', 'bathrooms', 'bedrooms', 'floorAreaSqM', 'livingRooms',
       'tenure', 'propertyType', 'currentEnergyRating', 'soldYear', 'soldT', 'soldPrice', 'sqmPrice', 'soldDate', 'size_bucket', 'construction_age_band', 'in_conservation_area']
df = df[column_order]


df['in_conservation_area'].value_counts()


# Null values
df_all_is_null = df[df['bedrooms'].isnull() & df['bathrooms'].isnull() & df['livingRooms'].isnull() & df['floorAreaSqM'].isnull()]
df = df.drop(df_all_is_null.index)
df = df.dropna(subset=['propertyType'])
df['tenure'] = df['tenure'].fillna('Unknown')
# df_floorAreaSqM_is_null = df[df['floorAreaSqM'].isnull()]
# df_bathrooms_is_null = df[df['bathrooms'].isnull()]
# df_bathrooms_is_null
# print(df_bathrooms_is_null.describe())

# df_allrooms_is_null = df[df['bedrooms'].isnull() & df['bathrooms'].isnull() & df['livingRooms'].isnull()]
# df_allrooms_is_null


# NULL MANAGEMENT
# FLOOR AREA NULL ---------------------------------------------------------------------------------------------------------------------------------------------

# 1. Create the 'area_code' column (extract letters, e.g., 'SE' from 'SE13')
df['area_code'] = df['outcode'].str.extract(r'^([A-Z]+)')

print(f"Starting Imputation. Initial nulls: {df['floorAreaSqM'].isnull().sum()}")

# STEP 1: Gold Standard (Exact Outcode)
# Matches: Type + Beds + Baths + Living + Exact Neighborhood (e.g., 'SE13')
# Added 'bedrooms' to the list
medians_gold = df.groupby(['propertyType', 'bedrooms', 'bathrooms', 'livingRooms', 'outcode'])['floorAreaSqM'].transform('median')
df['floorAreaSqM'] = df['floorAreaSqM'].fillna(medians_gold)

print(f"Nulls remaining after Step 1 (Exact Outcode): {df['floorAreaSqM'].isnull().sum()}")

# STEP 2: Silver Standard (Broad Area Code)
# Matches: Type + Beds + Baths + Living + Region (e.g., 'SE')

medians_silver = df.groupby(['propertyType', 'bedrooms', 'bathrooms', 'livingRooms', 'area_code'])['floorAreaSqM'].transform('median')
df['floorAreaSqM'] = df['floorAreaSqM'].fillna(medians_silver)

print(f"Nulls remaining after Step 2 (Area Code): {df['floorAreaSqM'].isnull().sum()}")
df = df.dropna(subset=['floorAreaSqM'])

# DROP WRONG COLLECTED DATA ---------------------------------------------------------------------------------------------------------------------------------------------
house_types = ['Detached House', 'Semi-Detached House', 'Mid Terrace House', 
               'End Terrace House', 'Terrace Property', 'Terraced']
df = df[
    (df['propertyType'].isin(['Purpose Built Flat', 'Flat/Maisonette', 'Converted Flat'])) |
    ((df['propertyType'].isin(house_types)) & (df['floorAreaSqM'] >= 120))]
df = df[df['sqmPrice'] < 70000]

# Drop duplicates
print(f"Rows BEFORE dropping duplicates: {len(df)}")
df = df.drop_duplicates(subset=['fullAddress', 'soldPrice', 'soldDate'], keep='first')
# Verify the result
print(f"Rows after dropping duplicates: {len(df)}")

rows_to_drop = (
    # Case 1: Tiny (< 37) with 2+ baths
    ((df['size_bucket'] == '< 37m²') & (df['bathrooms'] >= 2)) | 

    # Case 2: Small (37-50) with 2+ baths OR 2+ beds
    # IMPORTANT: The OR logic for beds/baths must be wrapped in its own brackets ()
    ((df['size_bucket'] == '37-50m²') & ((df['bathrooms'] >= 2) | (df['bedrooms'] >= 2))) | 

    # Case 3: Medium (50-60) with 3+ baths OR 4+ beds
    ((df['size_bucket'] == '50-60m²') & ((df['bathrooms'] >= 3) | (df['bedrooms'] >= 4))) | 

    # Case 4: Standard (60-72) with 4+ baths
    ((df['size_bucket'] == '60-72m²') & (df['bathrooms'] >= 4))
)
print(f"Dropping {rows_to_drop.sum()} rows.")
df = df[~rows_to_drop]


# BATHROOMS NULL---------------------------------------------------------------------------------------------------------------------------------------------

# # Added 1 bathroom to house <60 m2
small_house_mask = (df['bathrooms'].isnull()) & (df['floorAreaSqM'] < 60)
df.loc[small_house_mask, 'bathrooms'] = 1

df['area_code'] = df['outcode'].str.extract(r'^([A-Z]+)')

# Create 'Approximate Size' buckets (Rounding to nearest 5 meters)
df['sqm_approx'] = (df['floorAreaSqM'] / 5).round() * 5

print(f"Starting Imputation. Initial nulls in Bathrooms: {df['bathrooms'].isnull().sum()}")

# STEP 1: Gold Standard (Exact Outcode + Similar Size)
# Matches: Beds + Exact Outcode + Approx Size
print("Applying Step 1: Matching Exact Location & Size...")

medians_gold = df.groupby(['bedrooms', 'outcode', 'sqm_approx'])['bathrooms'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

df['bathrooms'] = df['bathrooms'].fillna(medians_gold)

print(f"Nulls remaining after Step 1: {df['bathrooms'].isnull().sum()}")

# STEP 2: Silver Standard (Broad Area Code + Similar Size)

print("Applying Step 2: Matching Broad Region & Size...")

# Note: We use the Broad 'area_code' here instead of specific 'outcode'
medians_silver = df.groupby(['bedrooms', 'area_code', 'sqm_approx'])['bathrooms'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

df['bathrooms'] = df['bathrooms'].fillna(medians_silver)

print(f"Nulls remaining after Step 2: {df['bathrooms'].isnull().sum()}")

# BEDROOMS and LIVING ROOMS NULL---------------------------------------------------------------------------------------------------------------------------------------------

# # All appartments smaller than 37 m2 will have 1 bedroom and 0 living rooms
studio_mask = df['floorAreaSqM'] < 38
print(f"Standardizing {studio_mask.sum()} properties < 36m² to be Studios (1 Bed, 0 Living).")
df.loc[studio_mask, 'bedrooms'] = 1
df.loc[studio_mask, 'livingRooms'] = 0

studio_mask2 = (df['floorAreaSqM'] > 38) & (df['floorAreaSqM'] < 57)
df.loc[studio_mask2, 'bedrooms'] = 1
df.loc[studio_mask2, 'livingRooms'] = 1

print(f"Starting Imputation. Initial nulls in Bathrooms: {df['bathrooms'].isnull().sum()}")

# STEP 1: Gold Standard (Exact Outcode + Similar Size)
# Matches: Beds + Living + Exact Outcode + Approx Size
print("Applying Step 1: Matching Exact Location & Size...")

medians_gold = df.groupby(['bathrooms', 'outcode', 'sqm_approx'])['bedrooms'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

df['bedrooms'] = df['bedrooms'].fillna(medians_gold)

print(f"Nulls remaining after Step 1: {df['bedrooms'].isnull().sum()}")

# STEP 2: Silver Standard (Broad Area Code + Similar Size)

print("Applying Step 2: Matching Broad Region & Size...")

# Note: We use the Broad 'area_code' here instead of specific 'outcode'
medians_silver = df.groupby(['bathrooms', 'area_code', 'sqm_approx'])['bedrooms'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

df['bedrooms'] = df['bedrooms'].fillna(medians_silver)

# STEP 1: Gold Standard (Exact Outcode + Similar Size)
# Matches: Beds + Living + Exact Outcode + Approx Size
print("Applying Step 1: Matching Exact Location & Size...")

medians_gold = df.groupby(['bedrooms', 'bathrooms', 'outcode', 'sqm_approx'])['livingRooms'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

df['livingRooms'] = df['livingRooms'].fillna(medians_gold)

print(f"Nulls remaining after Step 1: {df['livingRooms'].isnull().sum()}")

# STEP 2: Silver Standard (Broad Area Code + Similar Size)

print("Applying Step 2: Matching Broad Region & Size...")

# Note: We use the Broad 'area_code' here instead of specific 'outcode'
medians_silver = df.groupby(['bedrooms', 'bathrooms', 'area_code', 'sqm_approx'])['livingRooms'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

df['livingRooms'] = df['livingRooms'].fillna(medians_silver)
print(f"Nulls remaining after Step 1: {df['livingRooms'].isnull().sum()}")


# Energy Rates---------------------------------------------------------------------------------------------------------------------------------------------
df['currentEnergyRating'] = df['currentEnergyRating'].fillna('NotRated')

# Rest of nulls---------------------------------------------------------------------------------------------------------------------------------------------
df = df.dropna()


print(df['size_bucket'].unique())


print(df['bathrooms'].dtype)


df_incoerence1 = df[(df['size_bucket'] == '37-50m²')]
df_incoerence1.sort_values(by='bedrooms', ascending=False)


df.isnull().sum()


df


# # Comparison of general dataset with Null columns
# general_energy = df['currentEnergyRating'].value_counts(normalize=True)
# null_energy = df_floorAreaSqM_is_null['currentEnergyRating'].value_counts(normalize=True)

# comparison_energy = pd.DataFrame({
#     'General_Dataset': general_energy,
#     'Missing_FloorArea': null_energy
# })

# general_pt = df['propertyType'].value_counts(normalize=True)
# null_pt = df_floorAreaSqM_is_null['propertyType'].value_counts(normalize=True)

# comparison_pt = pd.DataFrame({
#     'General_Dataset': general_pt,
#     'Missing_FloorArea': null_pt
# })

# general_combined = df['soldYear'].astype(str) + "-T" + df['soldT'].astype(str)
# general_sold = general_combined.value_counts(normalize=True)

# null_combined = df_floorAreaSqM_is_null['soldYear'].astype(str) + "-T" + df_floorAreaSqM_is_null['soldT'].astype(str)
# null_sold = null_combined.value_counts(normalize=True)

# comparison_sold = pd.DataFrame({
#     'General_Dataset': general_sold,
#     'Missing_FloorArea': null_sold
# })

# comparison_sold = pd.DataFrame({
#     'General_Dataset': general_sold,
#     'Missing_FloorArea': null_sold
# })

# general_neighbourhood = df['outcode'].value_counts(normalize=True)
# null_neighbourhood = df_floorAreaSqM_is_null['outcode'].value_counts(normalize=True)

# comparison_neighbourhood = pd.DataFrame({
#     'General_Dataset': general_neighbourhood,
#     'Missing_FloorArea': null_neighbourhood
# })

# print(comparison_energy.sort_values(by='Missing_FloorArea', ascending=False))
# print(comparison_pt.sort_values(by='Missing_FloorArea', ascending=False))
# print(comparison_sold.sort_values(by='Missing_FloorArea', ascending=False))
# print(comparison_neighbourhood.sort_values(by='Missing_FloorArea', ascending=False).head(10))


# # 1. Create the subset for rows where bathrooms are null
# df_bathrooms_is_null = df[df['bathrooms'].isnull()]

# # --- 1. Energy Rating Analysis ---
# general_energy = df['currentEnergyRating'].value_counts(normalize=True)
# null_bath_energy = df_bathrooms_is_null['currentEnergyRating'].value_counts(normalize=True)

# comparison_energy = pd.DataFrame({
#     'General_Dataset': general_energy,
#     'Missing_Bathrooms': null_bath_energy
# })

# # --- 2. Property Type Analysis ---
# general_pt = df['propertyType'].value_counts(normalize=True)
# null_bath_pt = df_bathrooms_is_null['propertyType'].value_counts(normalize=True)

# comparison_pt = pd.DataFrame({
#     'General_Dataset': general_pt,
#     'Missing_Bathrooms': null_bath_pt
# })

# # --- 3. Date Sold Analysis ---
# # Create the time string for both
# general_combined = df['soldYear'].astype(str) + "-T" + df['soldT'].astype(str)
# general_sold = general_combined.value_counts(normalize=True)

# null_bath_combined = df_bathrooms_is_null['soldYear'].astype(str) + "-T" + df_bathrooms_is_null['soldT'].astype(str)
# null_bath_sold = null_bath_combined.value_counts(normalize=True)

# comparison_sold = pd.DataFrame({
#     'General_Dataset': general_sold,
#     'Missing_Bathrooms': null_bath_sold
# })

# # --- 4. Neighbourhood Analysis ---
# general_neighbourhood = df['outcode'].value_counts(normalize=True)
# null_bath_neighbourhood = df_bathrooms_is_null['outcode'].value_counts(normalize=True)

# comparison_neighbourhood = pd.DataFrame({
#     'General_Dataset': general_neighbourhood,
#     'Missing_Bathrooms': null_bath_neighbourhood
# })

# # --- 5. Floor Area Analysis (New) ---
# # Since Area is a number, we must group it into categories (bins) to compare percentages
# # Bins: <50 (Studio), 50-70 (Flat), 70-90 (Small House), 90-120 (Medium), >120 (Large)
# bins = [0, 50, 70, 90, 120, 10000]
# labels = ['< 50m²', '50-70m²', '70-90m²', '90-120m²', '> 120m²']

# # Create a temporary column for the buckets
# df['size_bucket'] = pd.cut(df['floorAreaSqM'], bins=bins, labels=labels)
# # Note: We need to re-create the null subset to include this new 'size_bucket' column
# df_bathrooms_is_null = df[df['bathrooms'].isnull()]

# general_sqm = df['size_bucket'].value_counts(normalize=True, sort=False)
# null_bath_sqm = df_bathrooms_is_null['size_bucket'].value_counts(normalize=True, sort=False)

# comparison_sqm = pd.DataFrame({
#     'General_Dataset': general_sqm,
#     'Missing_Bathrooms': null_bath_sqm
# })

# # --- PRINT RESULTS (Sorted by Missing %) ---
# # I added .fillna(0) because if a category has NO missing values, it appears as NaN, 
# # which messes up the sorting.

# print("--- Energy Ratings (Sorted by Missing Bathrooms %) ---")
# print(comparison_energy.fillna(0).sort_values(by='Missing_Bathrooms', ascending=False))
# print("\n")

# print("--- Property Type (Sorted by Missing Bathrooms %) ---")
# print(comparison_pt.fillna(0).sort_values(by='Missing_Bathrooms', ascending=False))
# print("\n")

# print("--- Date Sold (Sorted by Missing Bathrooms %) ---")
# print(comparison_sold.fillna(0).sort_values(by='Missing_Bathrooms', ascending=False))
# print("\n")

# print("--- Top 10 Neighbourhoods with Missing Bathrooms ---")
# print(comparison_neighbourhood.fillna(0).sort_values(by='Missing_Bathrooms', ascending=False).head(10))

# print("--- Property Size (Sorted by Missing Bathrooms %) ---")
# # Sorting by index so the sizes go from Small -> Large, which is easier to read than sorting by %
# print(comparison_sqm) 
# print("\n")

# # Quick Check: Are the 'Missing Bathroom' houses also 'Missing Area'?
# missing_both = df_bathrooms_is_null['floorAreaSqM'].isnull().mean()
# print(f"Percentage of rows missing Bathrooms that are ALSO missing Floor Area: {missing_both:.1%}")


(df.columns)


df.info()


plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='floorAreaSqM', bins=50, kde=True)
plt.title('Distribution of Floor Area (m²)')
plt.xlabel('Floor Area (m²)')
plt.ylabel('Count')
plt.show()


plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='sqmPrice', bins=500, kde=True)
plt.title('Distribution of sqmPrice (m²)')
plt.xlabel('sqmPrice (m²)')
plt.ylabel('Count')
plt.show()


df.describe()


df


df.shape


v_min = 0
v_max = 1700000

sns.set_style("white")

plt.figure(figsize=(12, 10)) # Square-ish aspect ratio works better for maps

sns.scatterplot(
    data=df, 
    x="longitude",      # Geographic X
    y="latitude",       # Geographic Y
    hue="soldPrice",    # Color based on price
    palette="magma",
    hue_norm=(v_min, v_max) # 'magma' or 'viridis' look great for geographic heatmaps
    alpha=0.3,          # Low intensity as requested
    edgecolor=None, 
    s=15                # Smaller dots often reveal the city shape better
)

# Visual cleanup for a background-style map
plt.title("London Real Estate Price Heatmap", fontsize=18, alpha=0.5, pad=20)
plt.axis('off') # Hiding axes often looks better for 'shape' based plots

# Optional: Add a very faint legend
plt.legend(title="Sold Price (£)", bbox_to_anchor=(1, 0.2), loc='lower right', frameon=False)

plt.tight_layout()
plt.show()

Using a dark background can sometimes make the colors 'pop' 
while keeping the overall intensity low, but white is safer for presentations.
sns.set_style("white")

plt.figure(figsize=(12, 10))

# 'Spectral_r' goes Blue -> Yellow -> Orange -> Red
# 'RdYlBu_r' is another great alternative for this look
sns.scatterplot(
    data=df, 
    x="longitude", 
    y="latitude", 
    hue="soldPrice", 
    palette="Spectral_r",
    hue_norm=(v_min, v_max)
    alpha=0.4,            # Soft background intensity
    edgecolor=None, 
    s=20                  # Adjusted for visibility
)

# Crucial for geographic accuracy: ensures London isn't stretched
plt.gca().set_aspect('equal', adjustable='box')

# Minimalist styling for presentation background
plt.axis('off') 
plt.title("London Property Value Heatmap", fontsize=18, color='gray', alpha=0.6)

# Customizing the legend to be subtle
plt.legend(title="Price", bbox_to_anchor=(1.05, 0.5), loc='center left', frameon=False)

plt.tight_layout()
plt.show()

# Define your price boundaries
v_min = 0
v_max = 1700000

plt.figure(figsize=(12, 10))

sns.scatterplot(
    data=df, 
    x="longitude", 
    y="latitude", 
    hue="soldPrice", 
    palette="Spectral_r", 
    hue_norm=(v_min, v_max), # Forces the color scale into your range
    alpha=0.2,
    edgecolor=None, 
    s=25
)

# Geographic correction
plt.gca().set_aspect('equal', adjustable='box')

# Cleanup
plt.axis('off')
plt.title("London Property Prices (Capped at £1.7M for contrast)", alpha=0.6)

plt.show()





# sns.pairplot(df, hue="soldPrice")
# plt.show()


df.info()


df = df.drop(columns=['soldDate'])


df.to_csv('../1_data/df_cleaned.csv', index=False)


df['in_conservation_area'].value_counts()


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from scipy.stats import pearsonr

# Create df_analysis
df_analysis = df.copy()

# A. Convert Rooms to Integers
room_cols = ['bathrooms', 'bedrooms', 'livingRooms']
df_analysis[room_cols] = df_analysis[room_cols].astype(int)

# B. Label Encode Energy Rating (Ordinal: A=7, G=1, NotRated=0)
energy_map = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1, 'NotRated': 0}
df_analysis['energy_encoded'] = df_analysis['currentEnergyRating'].map(energy_map)

# C. Target Encode Outcode (using median sqmPrice)
outcode_medians = df.groupby('outcode')['sqmPrice'].median()
df_analysis['neighborhood_value'] = df_analysis['outcode'].map(outcode_medians)

# D. One-Hot Encoding for categorical columns
# We drop the first category to avoid the "Dummy Variable Trap" (Multicollinearity)
df_analysis = pd.get_dummies(df_analysis, columns=['tenure', 'propertyType', 'soldT', 'construction_age_band'], drop_first=True)

# E. Select final features (excluding the original strings and size_bucket)
final_features = [
    'neighborhood_value', 'bathrooms', 'bedrooms', 'floorAreaSqM', 
    'livingRooms', 'energy_encoded', 'in_conservation_area'
] + [col for col in df_analysis.columns if 'tenure_' in col or 'propertyType_' in col or 'soldT_' in col or 'construction_age_band' in col]

X_analysis = df_analysis[final_features].astype(float)
y_target = df_analysis['soldPrice']

# 1. Heatmap
plt.figure(figsize=(12, 10))
corr_matrix = pd.concat([X_analysis, y_target], axis=1).corr()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap='coolwarm', center=0)
plt.title("Correlation Heatmap (Encoded Features)")
plt.show()

# 2. VIF Analysis
vif_data = pd.DataFrame()
X_vif = X_analysis.assign(const=1)
vif_data["feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(X_vif.columns))]
print(vif_data.sort_values(by="VIF", ascending=False))

comparison_list = []

for col in final_features:
    # Pearson r
    r_val, _ = pearsonr(X_analysis[col], y_target)
    
    # Polynomial R2 (Degree 2)
    poly = PolynomialFeatures(degree=2)
    X_poly = poly.fit_transform(X_analysis[[col]])
    poly_model = LinearRegression().fit(X_poly, y_target)
    r2_val = r2_score(y_target, poly_model.predict(X_poly))
    
    comparison_list.append({'Feature': col, 'Pearson': r_val, 'Poly_R2': r2_val})

comparison_df = pd.DataFrame(comparison_list).set_index('Feature').sort_values('Poly_R2', ascending=False)

# 4. Bar Plot Comparison
fig, axes = plt.subplots(1, 2, figsize=(16, 10), sharey=True)

# --- LEFT PLOT: Pearson (Direction) ---
sns.barplot(
    ax=axes[0], 
    x=comparison_df['Pearson'], 
    y=comparison_df.index, 
    hue=comparison_df.index, 
    palette='RdBu_r',  # Capitalized correctly now
    legend=False
)
axes[0].set_title('Linear Direction (Pearson $r$)', fontsize=14, fontweight='bold')
axes[0].axvline(0, color='black', linewidth=1)
axes[0].set_xlim(-1, 1)

# --- RIGHT PLOT: Polynomial R2 (Total Influence) ---
sns.barplot(
    ax=axes[1], 
    x=comparison_df['Poly_R2'], 
    y=comparison_df.index, 
    hue=comparison_df.index, 
    palette='viridis', # Lowercase is correct for this one
    legend=False
)
axes[1].set_title('Total Influence (Polynomial $R^2$)', fontsize=14, fontweight='bold')

# Add values to bars correctly
for i, (p, r2) in enumerate(zip(comparison_df['Pearson'], comparison_df['Poly_R2'])):
    # Pearson labels
    offset_p = 0.02 if p >= 0 else -0.12
    axes[0].text(p + offset_p, i, f"{p:.2f}", color='black', va='center', fontweight='bold')
    
    # R2 labels
    axes[1].text(r2 + 0.005, i, f"{r2:.3f}", color='black', va='center', fontweight='bold')

plt.tight_layout()
plt.show()


# %pip install statsmodels


df.columns


df_analysis.to_csv('../1_data/df_analysis.csv', index=False)


df_analysis


df_analysis.info()



