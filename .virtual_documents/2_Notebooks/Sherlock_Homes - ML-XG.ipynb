import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import xgboost as xgb
from scipy.stats import randint, uniform


# df_old_analysis = pd.read_csv(r"..\1_Data\df_analysis.csv")
df_original = pd.read_csv(r"..\1_Data\df_cleaned.csv")
df_analysis = df_original.copy()
df_ML = df_analysis.copy()


df_ML.info()


low = df_ML['sqmPrice'].quantile(0.01)
high = df_ML['sqmPrice'].quantile(0.95)
df_ML = df_ML[(df_ML['sqmPrice'] > low) & (df_ML['sqmPrice'] < high)]


df_ML.describe()


# A. Convert Rooms to Integers
room_cols = ['bathrooms', 'bedrooms', 'livingRooms']
df_ML[room_cols] = df_analysis[room_cols].astype(int)

# B. Label Encode Energy Rating (Ordinal: A=7, G=1, NotRated=0)
energy_map = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1, 'NotRated': np.nan}
df_ML['energy_encoded'] = df_ML['currentEnergyRating'].map(energy_map)

# C. One-Hot Encoding for categorical columns
# Drop the columns not needed
df_ML = df_ML.drop(columns=['soldT'])
df_ML = pd.get_dummies(df_ML, columns=['propertyType', 'tenure', 'construction_age_band'], drop_first=True, prefix=['prop', 'tenure', 'age'])

# D. Define features (X) and targets (y)
X = df_ML.drop(columns=['soldPrice']) 
y = np.log1p(df_ML['soldPrice'])


# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Target Encode Outcode (using median sqmPrice)
outcode_medians = X_train.groupby('outcode')['sqmPrice'].median()
global_median = X_train['sqmPrice'].median()
X_train['neighborhood_value'] = X_train['outcode'].map(outcode_medians)

X_test['neighborhood_value'] = X_test['outcode'].map(outcode_medians)
X_test['neighborhood_value'] = X_test['neighborhood_value'].fillna(global_median)

# E. Select final features (excluding the original strings and size_bucket)
final_features = [
    'neighborhood_value', 'bathrooms', 'bedrooms', 'floorAreaSqM', 
    'livingRooms', 'energy_encoded', 'tenure_Freehold', 'in_conservation_area', 'latitude', 'longitude'
] + [col for col in df_ML.columns if 'propertyType_' in col or 'age' in col.lower()]

X_train = X_train[final_features].astype(float)
X_test = X_test[final_features].astype(float)


# 1. Initialize XGBoost Regressor
# These parameters are "safe defaults" for regression
best_params = {
    'colsample_bytree': 0.6777095814048169,
    'learning_rate': 0.024188183399985533,
    'max_depth': 8,
    'min_child_weight': 3,
    'n_estimators': 670,
    'reg_alpha': 0.13808385936852352,
    'reg_lambda': 2.8943244775377934,
    'subsample': 0.7753896492072347,
    'random_state': 42  # Set this to ensure reproducibility
}

xg_reg = xgb.XGBRegressor(**best_params)

# 2. Train
print("Training XGBoost")
xg_reg.fit(X_train, y_train)

# 3. Predict
y_pred_log = xg_reg.predict(X_test)
# Clip extreme values to avoid overflow
y_pred_log = np.clip(y_pred_log, a_min=None, a_max=20)
# Inverse log transform
y_test_real = np.expm1(y_test)
y_pred_real = np.expm1(y_pred_log)

# Metrics
mse  = mean_squared_error(y_test_real, y_pred_real)
rmse = np.sqrt(mse)
mae  = mean_absolute_error(y_test_real, y_pred_real)
mape = mean_absolute_percentage_error(y_test_real, y_pred_real)
r2   = r2_score(y_test_real, y_pred_real)

print("-" * 30)
print("XGBoost Results (Cleaned Data):")
print(f"R² Score: {r2:.4f}")
print(f"RMSE:     £{rmse:,.0f}")
print(f"MAE:      £{mae:,.0f}")
print(f"MAPE:     {mape:.2%}")
print("-" * 30)





# Create a DataFrame for plotting
results = pd.DataFrame({
    'Actual': y_test_real,
    'Predicted': y_pred_real
})

# Calculate the error for each specific house
results['Error_Amount'] = results['Predicted'] - results['Actual']
results['Error_Percent'] = (results['Error_Amount'] / results['Actual']) * 100

plt.figure(figsize=(12, 6))

# Plot 1: Actual vs Predicted
plt.subplot(1, 2, 1)
sns.scatterplot(x='Actual', y='Predicted', data=results, alpha=0.3, color='blue')
# Draw a red line for "Perfect Prediction"
plt.plot([results['Actual'].min(), results['Actual'].max()], 
         [results['Actual'].min(), results['Actual'].max()], 
         color='red', lw=2, linestyle='--')
plt.title('Actual vs Predicted Prices')
plt.xlabel('Actual Price (£)')
plt.ylabel('Predicted Price (£)')
plt.xscale('log') # Log scale helps see the spread better
plt.yscale('log')

# Plot 2: How big is the error at different price points?
plt.subplot(1, 2, 2)
sns.scatterplot(x='Actual', y='Error_Percent', data=results, alpha=0.3, color='purple')
plt.axhline(0, color='red', linestyle='--')
plt.title('Percentage Error by Price')
plt.xlabel('Actual Price (£)')
plt.ylabel('Error (%)')
plt.xscale('log')
plt.ylim(-100, 100) # Zoom in to +/- 100% error

plt.tight_layout()
plt.show()

# Let's check the error specifically for houses around 500k
subset_500k = results[(results['Actual'] > 450000) & (results['Actual'] < 550000)]
avg_error_500k = subset_500k['Error_Amount'].abs().mean()
avg_mape_500k = subset_500k['Error_Percent'].abs().mean()

print(f"--- Analysis for ~£500k Houses ---")
print(f"Average Error Amount: £{avg_error_500k:,.0f}")
print(f"Average Error %:      {avg_mape_500k:.2f}%")


# #  GRID SEARCH + CROSS VALIDATION

# # 1. Define the Parameter Grid (The options we want to try)
# param_grid = {
#     'max_depth': [3, 5, 7],           # 3 is simple, 7 is complex
#     'learning_rate': [0.01, 0.05, 0.1], # 0.01 is slow/precise, 0.1 is fast
#     'n_estimators': [500, 1000],      # Number of trees
#     'subsample': [0.8, 1.0]           # 0.8 means "ignore 20% of data randomly" (helps with outliers!)
# }

# # 2. Initialize the Model
# xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42)

# # 3. Setup the Grid Search
# # cv=3 means "Cross Validation": it splits data 3 times to ensure the result isn't luck.
# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, 
#                            cv=3, scoring='neg_mean_absolute_error', verbose=1)

# # 4. Train (This will run roughly 3 * 3 * 2 * 2 * 3 = 108 fits!)
# print("Starting Hyperparameter Tuning... (Go grab a coffee ☕)")
# grid_search.fit(X_train, y_train)

# # 5. Get the Best Results
# best_params = grid_search.best_params_
# print("-" * 30)
# print(f"Best Parameters Found: {best_params}")
# print("-" * 30)

# # 6. Evaluate the Best Model
# best_model_gridCV = grid_search.best_estimator_
# y_pred_log_gridCV = best_model_gridCV.predict(X_test)
# # Reverse the Log Transformation (expm1 = e^x - 1)
# y_pred_real_gridCV = np.expm1(y_pred_log_gridCV)
# y_test_real_gridCV = np.expm1(y_test)

# # General Metrics
# r2_gridCV = r2_score(y_test_real_gridCV, y_pred_real_gridCV)
# rmse_gridCV = np.sqrt(mean_squared_error(y_test_real_gridCV, y_pred_real_gridCV))
# mae_gridCV = mean_absolute_error(y_test_real_gridCV, y_pred_real_gridCV)
# mape_gridCV = mean_absolute_percentage_error(y_test_real_gridCV, y_pred_real_gridCV)

# print("Results with Tuned Hyperparameters:")
# print(f"R² Score: {r2:.4f}")
# print(f"RMSE:     £{rmse:,.0f}")
# print(f"MAE:      £{mae:,.0f}")
# print(f"MAPE:     {mape:.2%}")


# # RANDOM SEARCH + CROSS VALIDATION

# # 1. Define the Parameter Distribution
# # We widen the search space to include Regularization (alpha/lambda) to fight outliers
# param_dist = {
#     'n_estimators': randint(200, 1000),      # Any number of trees between 200 and 1000
#     'max_depth': randint(3, 10),             # Depths between 3 and 10
#     'learning_rate': uniform(0.01, 0.2),     # Any rate between 0.01 and 0.21
#     'subsample': uniform(0.6, 0.4),          # Randomly choose between 0.6 and 1.0
#     'colsample_bytree': uniform(0.6, 0.4),   # Randomly choose % of columns per tree
#     'min_child_weight': randint(1, 10),      # CRITICAL: Controls outlier sensitivity
#     'reg_alpha': uniform(0, 5),              # L1 Regularization (removes noise)
#     'reg_lambda': uniform(0, 5)              # L2 Regularization (reduces extreme weights)
# }

# # 2. Initialize Base Model
# # n_jobs=1 here because the RandomizedSearchCV will handle parallel processing
# xgb_base = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=1)

# # 3. Setup Random Search
# # n_iter=50 means we try 50 random combinations
# random_search_xg = RandomizedSearchCV(
#     estimator=xgb_base, 
#     param_distributions=param_dist, 
#     n_iter=50, 
#     cv=3, 
#     scoring='neg_mean_absolute_error', 
#     verbose=1, 
#     n_jobs=-1, 
#     random_state=42
# )

# # 4. Train (Fit)
# print("Starting XGBoost Random Search (Searching for the perfect noise settings)...")
# random_search_xg.fit(X_train, y_train)

# # 5. Get the Winner
# best_xg_rand = random_search_xg.best_estimator_
# print("-" * 30)
# print(f"Best Parameters: {random_search_xg.best_params_}")
# print("-" * 30)

# # 6. Predict
# y_pred_log_xg_randCV = best_xg_rand.predict(X_test)

# # 7. Reverse Log
# y_test_real_xg_randCV = np.expm1(y_test)
# y_pred_real_xg_randCV = np.expm1(y_pred_log_xg_randCV)

# # 8. Metrics
# mse_xg_randCV  = mean_squared_error(y_test_real_xg_randCV, y_pred_real_xg_randCV)
# rmse_xg_randCV = np.sqrt(mse_xg_randCV)
# mae_xg_randCV  = mean_absolute_error(y_test_real_xg_randCV, y_pred_real_xg_randCV)
# mape_xg_randCV = mean_absolute_percentage_error(y_test_real_xg_randCV, y_pred_real_xg_randCV)
# r2_xg_randCV   = r2_score(y_test_real_xg_randCV, y_pred_real_xg_randCV)

# # 9. Display
# print("Model Performance (XGBoost Random Search):")
# print("-" * 30)
# print(f"R² Score: {r2_xg_randCV:.4f}")
# print(f"RMSE:     £{rmse_xg_randCV:,.0f}")
# print(f"MAE:      £{mae_xg_randCV:,.0f}")
# print(f"MAPE:     {mape_xg_randCV:.2%}")
# print("-" * 30)


import shap

# 1. Initialize the SHAP Explainer using your best model
# Use a sample of X_test to speed up calculation
explainer = shap.TreeExplainer(xg_reg)
X_sample = X_test.sample(1000, random_state=42)
shap_values = explainer.shap_values(X_sample)

# 2. Plot the Summary
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_sample, plot_type="dot")



