import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import xgboost as xgb
from scipy.stats import randint, uniform


# df_old_analysis = pd.read_csv(r"..\1_Data\df_analysis.csv")
df_original = pd.read_csv(r"..\1_Data\df_cleaned.csv")
df_analysis = df_original.copy()
df_ML = df_analysis.copy()


pd.set_option('display.max_columns', None)
df_ML


low = df_ML['sqmPrice'].quantile(0.01)
high = df_ML['sqmPrice'].quantile(0.95)
df_ML = df_ML[(df_ML['sqmPrice'] > low) & (df_ML['sqmPrice'] < high)]


df_ML.describe()


# %pip install lightgbm


import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import matplotlib.pyplot as plt
import seaborn as sns

# # Outlier filtering (sqmPrice 0.01 to 0.95)
# low, high = df_ML['sqmPrice'].quantile([0.01, 0.95])
# df_ML = df_ML[(df_ML['sqmPrice'] > low) & (df_ML['sqmPrice'] < high)].copy()

# 2. Native Categorical Prep
# Instead of dummies, we convert columns to the 'category' dtype
cat_features = ['propertyType', 'tenure', 'construction_age_band', 'currentEnergyRating']
for col in cat_features:
    df_ML[col] = df_ML[col].astype('category')

# 3. Numeric Prep
room_cols = ['bathrooms', 'bedrooms', 'livingRooms']
df_ML[room_cols] = df_ML[room_cols].fillna(0).astype(int)

# 4. Target and Feature Selection
# We keep the original strings! LightGBM will handle them.
X = df_ML.drop(columns=['soldPrice', 'soldT']) 
y = np.log1p(df_ML['soldPrice'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Target Encode Outcode (Neighborhood Value)
outcode_medians = X_train.groupby('outcode')['sqmPrice'].median()
global_median = X_train['sqmPrice'].median()
X_train['neighborhood_value'] = X_train['outcode'].map(outcode_medians)
X_test['neighborhood_value'] = X_test['outcode'].map(outcode_medians).fillna(global_median)

# 6. Final Feature Selection
# Notice we are including the categorical strings directly now
features = [
    'neighborhood_value', 'bathrooms', 'bedrooms', 'floorAreaSqM', 
    'livingRooms', 'in_conservation_area', 'latitude', 'longitude',
    'propertyType', 'tenure', 'construction_age_band', 'currentEnergyRating'
]

X_train_final = X_train[features]
X_test_final = X_test[features]

# 7. LightGBM with Native Categorical Support
best_lgbm_params = {
    'colsample_bytree': 0.736249827785696,
    'learning_rate': 0.036069452823590316,
    'max_depth': 11,
    'min_child_samples': 13,
    'n_estimators': 1325,
    'num_leaves': 133,
    'reg_alpha': 3.14471423389942,
    'reg_lambda': 4.387360067635265,
    'subsample': 0.8940284175215543,
    'random_state': 42,
    'n_jobs': -1,
    'importance_type': 'gain'
}

lgb_reg = lgb.LGBMRegressor(**best_lgbm_params)

# IMPORTANT: We tell the model which columns are categorical
print("Training LightGBM with Native Categorical Support...")
lgb_reg.fit(
    X_train_final, y_train,
    categorical_feature=cat_features
)

# 8. Evaluation
y_pred_log = lgb_reg.predict(X_test_final)
y_test_real = np.expm1(y_test)
y_pred_real = np.expm1(y_pred_log)

# Metrics
print("-" * 30)
print(f"LGBM R² Score: {r2_score(y_test_real, y_pred_real):.4f}")
print(f"LGBM MAPE:     {mean_absolute_percentage_error(y_test_real, y_pred_real):.2%}")
print("-" * 30)


# Create a DataFrame for plotting
results = pd.DataFrame({
    'Actual': y_test_real,
    'Predicted': y_pred_real
})

# Calculate the error for each specific house
results['Error_Amount'] = results['Predicted'] - results['Actual']
results['Error_Percent'] = (results['Error_Amount'] / results['Actual']) * 100

plt.figure(figsize=(12, 6))

# Plot 1: Actual vs Predicted
plt.subplot(1, 2, 1)
sns.scatterplot(x='Actual', y='Predicted', data=results, alpha=0.3, color='blue')
# Draw a red line for "Perfect Prediction"
plt.plot([results['Actual'].min(), results['Actual'].max()], 
         [results['Actual'].min(), results['Actual'].max()], 
         color='red', lw=2, linestyle='--')
plt.title('Actual vs Predicted Prices')
plt.xlabel('Actual Price (£)')
plt.ylabel('Predicted Price (£)')
plt.xscale('log') # Log scale helps see the spread better
plt.yscale('log')

# Plot 2: How big is the error at different price points?
plt.subplot(1, 2, 2)
sns.scatterplot(x='Actual', y='Error_Percent', data=results, alpha=0.3, color='purple')
plt.axhline(0, color='red', linestyle='--')
plt.title('Percentage Error by Price')
plt.xlabel('Actual Price (£)')
plt.ylabel('Error (%)')
plt.xscale('log')
plt.ylim(-100, 100) # Zoom in to +/- 100% error

plt.tight_layout()
plt.show()

# Let's check the error specifically for houses around 500k
subset_500k = results[(results['Actual'] > 450000) & (results['Actual'] < 550000)]
avg_error_500k = subset_500k['Error_Amount'].abs().mean()
avg_mape_500k = subset_500k['Error_Percent'].abs().mean()

print(f"--- Analysis for ~£500k Houses ---")
print(f"Average Error Amount: £{avg_error_500k:,.0f}")
print(f"Average Error %:      {avg_mape_500k:.2f}%")


import shap

# 1. Initialize the SHAP Explainer
explainer = shap.TreeExplainer(lgb_reg)

# 2. Filter the sample to ONLY the features used in the model
# We take the sample from X_test_final, which is already cleaned!
X_sample = X_test_final.sample(1000, random_state=42)

# 3. Calculate SHAP values
# We use check_additivity=False to avoid small log-transform rounding errors
shap_values = explainer.shap_values(X_sample, check_additivity=False)

# 4. Plot the Summary
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_sample, plot_type="dot")


# # RANDOM SEARCH + CROSS VALIDATION

# from sklearn.model_selection import RandomizedSearchCV
# from scipy.stats import randint, uniform
# import lightgbm as lgb

# # 1. Define the Parameter Space
# # Note: LightGBM uses 'num_leaves' as the main complexity control
# param_dist_lgb = {
#     'n_estimators': randint(500, 1500),
#     'learning_rate': uniform(0.01, 0.1),
#     'num_leaves': randint(20, 150),          # The most important LGBM parameter
#     'max_depth': randint(5, 15),             # Can still be used to prevent deep-tree overfitting
#     'min_child_samples': randint(10, 50),    # Equivalent to min_child_weight
#     'subsample': uniform(0.6, 0.4),
#     'colsample_bytree': uniform(0.6, 0.4),
#     'reg_alpha': uniform(0, 5),
#     'reg_lambda': uniform(0, 5)
# }

# # 2. Initialize the Base Model
# lgb_base = lgb.LGBMRegressor(random_state=42, n_jobs=-1)

# # 3. Setup the Random Search
# # Use the same 'neg_mean_absolute_error' you used for XGBoost
# random_search_lgb = RandomizedSearchCV(
#     estimator=lgb_base,
#     param_distributions=param_dist_lgb,
#     n_iter=50,
#     cv=3,
#     scoring='neg_mean_absolute_error',
#     verbose=1,
#     n_jobs=-1,
#     random_state=42
# )

# # 4. Run the Search
# print("Starting LightGBM Random Search...")
# # If using the 'Definitive' categorical code, pass the category names here
# random_search_lgb.fit(
#     X_train_final, y_train,
#     categorical_feature=['propertyType', 'tenure', 'construction_age_band', 'currentEnergyRating']
# )

# # 5. Review Results
# print("-" * 30)
# print(f"Best LGBM Parameters: {random_search_lgb.best_params_}")
# print(f"Best Score (MAE): {-random_search_lgb.best_score_:.4f}")
# print("-" * 30)





import pickle

# 1. Create the Lookup Dictionaries
# Map 'outcode' -> 'neighborhood_value' (Price per sqm median)
# We use the dictionary you created during training logic
neighborhood_dict = X_train.groupby('outcode')['sqmPrice'].median().to_dict()

# Map 'outcode' -> 'Latitude/Longitude'
# We use the full original clean dataframe for this to get maximum coverage
coords_dict = df.groupby('outcode')[['latitude', 'longitude']].mean().to_dict('index')

# 2. Get the Valid Options for Dropdowns
# We use df_ML (the final filtered data) to ensure we don't offer options the model removed
# (e.g., If you removed all 'Detached Houses', don't show it in the app)
dropdown_options = {
    'propertyType': list(df_ML['propertyType'].unique()),
    'tenure': list(df_ML['tenure'].unique()),
    'construction_age_band': list(df_ML['construction_age_band'].unique()),
    'currentEnergyRating': list(df_ML['currentEnergyRating'].unique()),
    'global_median_price': df_ML['sqmPrice'].median() # Fallback
}

# 3. Define the EXACT feature order the model expects
# This MUST match X_train_final.columns exactly
features = [
    'neighborhood_value', 'bathrooms', 'bedrooms', 'floorAreaSqM', 
    'livingRooms', 'in_conservation_area', 'latitude', 'longitude',
    'propertyType', 'tenure', 'construction_age_band', 'currentEnergyRating'
]

# 4. Save everything
model_payload = {
    'model': lgb_reg,
    'neighborhood_map': neighborhood_dict,
    'coords_map': coords_dict,
    'options': dropdown_options,
    'features': features
}

with open('london_real_estate_brain.pkl', 'wb') as f:
    pickle.dump(model_payload, f)

print("✅ Brain saved! ready for the App.")



