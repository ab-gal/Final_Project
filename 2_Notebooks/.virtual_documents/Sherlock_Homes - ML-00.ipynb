import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from scipy.stats import pearsonr


df_analysis = pd.read_csv(r"..\1_Data\df_analysis.csv")
df_original = pd.read_csv(r"..\1_Data\df_cleaned.csv")
df_ML = df_analysis.copy()


df_ML


# from sklearn.ensemble import RandomForestRegressor
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import r2_score, mean_absolute_error

# # 1. Select only the physical and neighborhood-value columns
# base_features = [
#     'bathrooms', 
#     'bedrooms', 
#     'floorAreaSqM', 
#     'livingRooms', 
#     'energy_encoded', 
#     'neighborhood_value'
# ]

# # 2. Add the Property Type dummy columns automatically
# property_type_features = [col for col in df_ML.columns if 'propertyType_' in col]

# # 3. Combine them
# final_features = base_features + property_type_features

# X = df_ML[final_features].astype(float) # Convert bools to 1.0/0.0 for the model
# y = df_ML['soldPrice']

# print(f"Model will train on {len(final_features)} features.")


# from sklearn.ensemble import RandomForestRegressor
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import r2_score, mean_absolute_error

# # Split the data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Initialize and fit
# rf = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)
# rf.fit(X_train, y_train)

# # Predict
# y_pred = rf.predict(X_test)

# # Evaluation
# print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")
# print(f"Mean Error: £{mean_absolute_error(y_test, y_pred):,.2f}")


# import numpy as np
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error

# # 1. Split the data (X and y stay as they are)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # 2. Transform the training target to Log scale
# y_train_log = np.log1p(y_train)

# # 3. Initialize and fit using the LOG target
# rf = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)
# rf.fit(X_train, y_train_log)

# # 4. Predict (output will be in Log scale)
# y_pred_log = rf.predict(X_test)

# # 5. Convert predictions back to original Pounds scale
# y_pred = np.expm1(y_pred_log)

# # 6. Evaluation
# # Note: R2 is still calculated on original y_test vs back-transformed y_pred
# print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")
# print(f"Mean Error (MAE): £{mean_absolute_error(y_test, y_pred):,.2f}")
# print(f"Mean Percentage Error (MAPE): {mean_absolute_percentage_error(y_test, y_pred):.2%}")


# importances = pd.DataFrame({
#     'Feature': X.columns,
#     'Importance': rf.feature_importances_
# }).sort_values(by='Importance', ascending=False)

# plt.figure(figsize=(10, 6))
# sns.barplot(x='Importance', y='Feature', data=importances, palette='viridis', hue='Feature', legend=False)
# plt.title("Random Forest: Feature Importance", fontsize=14, fontweight='bold')
# plt.xlabel("Importance (Relative Contribution to Price)")
# plt.show()


import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error

# 1. Create the Luxury Flag (Safely, using Neighborhood Value)
# We use 15k/sqm as your threshold for a 'Luxury Neighborhood'
df_ML['is_luxury_area'] = (df_ML['neighborhood_value'] > 15000).astype(int)

# 2. Define Features (Ensure is_luxury_area is included)
base_features = [
    'bathrooms', 'bedrooms', 'floorAreaSqM', 'livingRooms', 
    'energy_encoded', 'neighborhood_value', 'is_luxury_area'
]
property_type_features = [col for col in df_ML.columns if 'propertyType_' in col]
final_features = base_features + property_type_features

X = df_ML[final_features].astype(float)
y = df_ML['soldPrice']

# 3. Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Log Transform and Train
y_train_log = np.log1p(y_train)
rf = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train_log)

# 5. Predict and Un-log
y_pred_log = rf.predict(X_test)
y_pred = np.expm1(y_pred_log)

# 6. Final Results
print(f"--- Final Model Results ---")
print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")
print(f"Mean Error (MAE): £{mean_absolute_error(y_test, y_pred):,.2f}")
print(f"Mean Percentage Error (MAPE): {mean_absolute_percentage_error(y_test, y_pred):.2%}")
